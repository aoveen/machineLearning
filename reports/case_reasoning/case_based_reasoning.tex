\documentclass[10pt,a4paper]{article}
\usepackage[margin=1.4cm]{geometry}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{morefloats}

\setlength{\parskip}{5pt}
\setlength{\parindent}{0pt}

\author{Hector Dearman \and Paul Rowe-White \and Kritaphat Songsriin \and Simon Stuckemann}
\title{Machine Learning CBC: Case-Based Reasoning\\Group 2}
\begin{document}
\maketitle

\section{Case-based Reasoning}
Contrary to the previous algorithms we have implemented (decision tree learning and artificial neural networks) which belong to the class of \emph{eager learning algorithms}, instance-based learning algorithms belong to the class  of so-called \emph{lazy learning methods}. 
While \emph{eager learning algorithms} construct a general and explicit description of the target function when training examples are provided, \emph{lazy learning algorithms}, such as \emph{Case-based reasoning} simply store the training examples. Generalising beyond these examples is only done when a new instance must be classified. When a new instance should be classified, the algorithm compares the instance to the existing instances and assigns a target function value (the classification) based on the instance's relation to the already stored values. After classification of the new example, the \emph{CBR} system stores ("retains") the new example in order to use it for future classification. For this reason, the \emph{case-based} reasoning algorithm is also known as an \emph{online}-learning method as the algorithm continues to learn throughout its lifetime.

\section{Implementation Details}
Describe implemenation of CBR cases, the retrieve, reuse and retain functions. Flowchart of how the code works.

Describe how you initialise your CBR system.

\subsection{Dealing with Existing Cases}

It is possible that during the initial training phase but also later on when retaining an example, the case-based reasoning algorithm is tasked with adding a case to the system that already exists. We decided to take a similar approach to handling this case as described in \cite{Pantic2004}. In this paper, the authors describe how examples with higher typicality than others are given a higher weight than others. What this means is that if the CBR is contains two distinct examples A and B with typicality a and b respectively and $a > b$, the system will classify a new example C the same as example A, given that both A and B are returned as a possible best match by the algorithm. 
In order to implement this, we decided against using an explicit \emph{typicality} field in the \emph{Case} structure. Instead, we simply add the case multiple times, thus implicitly increasing the typicality of an example. 

\subsection{Dealing with Multiple Best Matches}
As described in the previous section, we add multiple instances of the same example if a case already exists. This way, we implicitly increase the typicality of that case. As outlined above, when retrieving the best match we want to give advantage to best matches that have a higher typicality since they are more likely to be correct classifications. The way we give this advantage is again implicit.We first calculate the best match in terms of the case's performance using the distance measure. Since multiple cases can have the same performance a set of possible best matches will be returned. Since we do not explicitly handle the case when a case already exists this set of possible best matches will thus contain a number of duplicate entries. We then proceed by taking a random element out of this set as our final answer, thus implicitly weighing cases with higher typicality higher than others.

\subsection{Similarity Measures}
Compare the different similarity measures you have used (at least three). What are the advantages/ disadvantages of each measure?

\section{Performance Results}

Is there any difference in the CBR performance on noisy and clean data. If yes, why?

Commented results of the average confusion matrix, average classification rate and recall, precision and F1 measures per class.

Table~\ref{tab:avgClassificationRate} shows that there is a difference in performance of CBR on noisy and clean data. This difference is a 10\% - 15\% reduction in average classification rate for noisy data compared to clean data. A reason for this can be that the the noisy data contains attributes that are not relevant to solving the problem (e.g. examples that have the same set of 'core' attributes that can be used to classify it but also have different sets of other attributes that). This means these examples will be spread out when they should actually be close together, and so when a new example needs classifying the examples it should use might be far away from it. The top performing measure, Shared AUs, does not take into account what AUs are important for classification so it may indicate that two examples are similar if they share the same irrelevant AUs.


\begin{table}[!ht]
\centering
\begin{tabular}{|l|l|c|c|}
	\cline{3-4}
	\multicolumn{2}{c}{}& \multicolumn{2}{ |c| }{Average Classification Rate}\\
	\cline{3-4}
	\multicolumn{2}{c|}{} & Clean & Noisy \\ \cline{1-4}
	\multirow{10}{*}{Similarity Measure}& Dice Dissimilarity & 0.809 & 0.618   \\ \cline{2-4}
	& Jaccard & 0.809 & 0.618  \\ \cline{2-4}
	& Matching Dissimilarity & 0.86 & 0.703   \\ \cline{2-4}
	& RR Dissimilarity & 0.726  & 0.593  \\ \cline{2-4}
	& RT Dissimilarity & 0.86  & 0.703   \\ \cline{2-4}	
	& Rubbish & 0.193 & 0.187  \\ \cline{2-4}
	& Shared AUs & 0.86 & 0.703   \\ \cline{2-4}
	& SS Dissimilarity & 0.809 & 0.618  \\ \cline{2-4}
	& Weighted & 0.867 & 0.682   \\ \cline{2-4}
	& Yule Dissimilarity & 0.796 & 0.595\\ \hline
\end{tabular}
\caption{Average Classification Rate}
\label{tab:avgClassificationRate}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{|l|l|c|c|c|c|c|c|}
	\cline{3-8}
	\multicolumn{2}{c}{}& \multicolumn{6}{ |c| }{Predicted class}\\
	\cline{3-8}
	\multicolumn{2}{c|}{} & 1 Anger & 2 Disgust & 3 Fear & 4 Happiness & 5 Sadness & 6 Surprise\\ \cline{1-8}
	\multirow{6}{*}{Actual class}& 1 Anger & 110 & 8 & 4 & 2 & 6 & 1 \\ \cline{2-8}
	& 2 Disgust & 14 & 161 & 1 & 8 & 13 & 1\\ \cline{2-8}
	& 3 Fear & 7 & 6 & 95 & 0 & 0 & 10 \\ \cline{2-8}
	& 4 Happiness & 0 & 9 & 0 & 204 & 0 & 2 \\ \cline{2-8}
	& 5 Sadness & 8 & 16 & 2 & 4 & 101 & 1 \\ \cline{2-8}
	& 6 Surprise & 1 & 3 & 9 & 3 & 0 & 189\\ \hline
\end{tabular}
\caption{Confusion Matrix - Shared AUs - Clean Data}
\label{tab:sharedAUsCleanConfusion}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{|l|l|c|c|c|c|c|c|}
	\cline{3-8}
	\multicolumn{2}{c}{}& \multicolumn{6}{ |c| }{Predicted class}\\
	\cline{3-8}
	\multicolumn{2}{c|}{} & 1 Anger & 2 Disgust & 3 Fear & 4 Happiness & 5 Sadness & 6 Surprise\\ \cline{1-8}
	\multirow{6}{*}{Actual class}& 1 Anger & 40 & 13 & 17 & 4 & 10 & 4 \\ \cline{2-8}
	& 2 Disgust & 15 & 148 & 16 & 3 & 4 & 1\\ \cline{2-8}
	& 3 Fear & 17 & 16 & 120 & 11 & 7 & 16\\ \cline{2-8}
	& 4 Happiness & 13 & 10 & 13 & 163 & 1 & 8 \\ \cline{2-8}
	& 5 Sadness & 26 & 12 & 7 & 3 & 57 & 5 \\ \cline{2-8}
	& 6 Surprise & 9 & 4 & 16 & 8 & 8 & 175\\ \hline
\end{tabular}
\caption{Confusion Matrix - Shared AUs - Noisy Data}
\label{tab:sharedAUsNoisyConfusion}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{|l|l|c|c|c|c|c|c|}
	\cline{3-8}
	\multicolumn{2}{c}{}& \multicolumn{6}{ |c| }{Predicted class}\\
	\cline{3-8}
	\multicolumn{2}{c|}{} & 1 Anger & 2 Disgust & 3 Fear & 4 Happiness & 5 Sadness & 6 Surprise\\ \cline{1-8}
	\multirow{6}{*}{Actual class}& 1 Anger & 103 & 11 & 4 & 2 & 10 & 1 \\ \cline{2-8}
	& 2 Disgust & 12 & 173 & 0 & 2 & 10 & 1\\ \cline{2-8}
	& 3 Fear & 7 & 7 & 95 & 2 & 0 & 7 \\ \cline{2-8}
	& 4 Happiness & 0 & 9 & 1 & 204 & 0 & 1 \\ \cline{2-8}
	& 5 Sadness & 13 & 15 & 3 & 1 & 100 & 0 \\ \cline{2-8}
	& 6 Surprise & 1 & 2 & 5 & 6 & 0 & 192\\ \hline
\end{tabular}
\caption{Confusion Matrix - Weighted - Clean Data}
\label{tab:weightedCleanConfusion}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{|l|l|c|c|c|c|c|c|}
	\cline{3-8}
	\multicolumn{2}{c}{}& \multicolumn{6}{ |c| }{Predicted class}\\
	\cline{3-8}
	\multicolumn{2}{c|}{} & 1 Anger & 2 Disgust & 3 Fear & 4 Happiness & 5 Sadness & 6 Surprise\\ \cline{1-8}
	\multirow{6}{*}{Actual class}& 1 Anger & 34 & 21 & 19 & 2 & 9 & 3 \\ \cline{2-8}
	& 2 Disgust & 24 & 138 & 12 & 4 & 6 & 3\\ \cline{2-8}
	& 3 Fear & 12 & 17 & 118 & 11 & 8 & 21 \\ \cline{2-8}
	& 4 Happiness & 12 & 12 & 11 & 162 & 1 & 10 \\ \cline{2-8}
	& 5 Sadness & 25 & 17 & 13 & 1 & 48 & 6 \\ \cline{2-8}
	& 6 Surprise & 4 & 4 & 13 & 10 & 7 & 182\\ \hline
\end{tabular}
\caption{Confusion Matrix - Weighted - Noisy Data}
\label{tab:weightedNoisyConfusion}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{|l|l|c|c|c|c|c|c|}
	\cline{3-8}
	\multicolumn{2}{c}{}& \multicolumn{6}{ |c| }{Predicted class}\\
	\cline{3-8}
	\multicolumn{2}{c|}{} & 1 Anger & 2 Disgust & 3 Fear & 4 Happiness & 5 Sadness & 6 Surprise\\ \cline{1-8}
	\multirow{6}{*}{Actual class}& 1 Anger & 104 & 12 & 5 & 2 & 6 & 2 \\ \cline{2-8}
	& 2 Disgust & 23 & 152 & 3 & 9 & 10 & 1\\ \cline{2-8}
	& 3 Fear & 11 & 4 & 87 & 1 & 0 & 15 \\ \cline{2-8}
	& 4 Happiness & 5 & 6 & 4 & 197 & 0 & 3 \\ \cline{2-8}
	& 5 Sadness & 11 & 21 & 8 & 4 & 86 & 2 \\ \cline{2-8}
	& 6 Surprise & 3 & 3 & 12 & 5 & 0 & 183\\ \hline
\end{tabular}
\caption{Confusion Matrix - Dice Dissimilarity - Clean Data}
\label{tab:diceCleanConfusion}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{|l|l|c|c|c|c|c|c|}
	\cline{3-8}
	\multicolumn{2}{c}{}& \multicolumn{6}{ |c| }{Predicted class}\\
	\cline{3-8}
	\multicolumn{2}{c|}{} & 1 Anger & 2 Disgust & 3 Fear & 4 Happiness & 5 Sadness & 6 Surprise\\ \cline{1-8}
	\multirow{6}{*}{Actual class}& 1 Anger & 25 & 21 & 26 & 5 & 6 & 5 \\ \cline{2-8}
	& 2 Disgust & 17 & 150 & 8 & 6 & 3 & 3\\ \cline{2-8}
	& 3 Fear & 23 & 22 & 102 & 14 & 3 & 23 \\ \cline{2-8}
	& 4 Happiness & 8 & 20 & 20 & 144 & 2 & 14 \\ \cline{2-8}
	& 5 Sadness & 19 & 15 & 20 & 7 & 29 & 20 \\ \cline{2-8}
	& 6 Surprise & 6 & 11 & 19 & 10 & 6 & 168\\ \hline
\end{tabular}
\caption{Confusion Matrix -  Dice Dissimilarity - Noisy Data}
\label{tab:diceNoisyConfusion}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{|l|l|c|c|c|c|c|c|}
	\cline{3-8}
	\multicolumn{2}{c}{}& \multicolumn{6}{ |c| }{Predicted class}\\
	\cline{3-8}
	\multicolumn{2}{c|}{} & 1 Anger & 2 Disgust & 3 Fear & 4 Happiness & 5 Sadness & 6 Surprise\\ \cline{1-8}
	\multirow{6}{*}{Actual class}& 1 Anger & 69 & 16 & 6 & 1 & 38 & 1 \\ \cline{2-8}
	& 2 Disgust & 8 & 172 & 4 & 2 & 12 & 0\\ \cline{2-8}
	& 3 Fear & 4 & 7 & 85 & 1 & 13 & 8 \\ \cline{2-8}
	& 4 Happiness & 2 & 13 & 2 & 189 & 7 & 2 \\ \cline{2-8}
	& 5 Sadness & 1 & 22 & 3 & 1 & 104 & 1 \\ \cline{2-8}
	& 6 Surprise & 1 & 4 & 12 & 4 & 8 & 177\\ \hline
\end{tabular}
\caption{Confusion Matrix - Yule Dissimilarity - Clean Data}
\label{tab:yuleCleanConfusion}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{|l|l|c|c|c|c|c|c|}
	\cline{3-8}
	\multicolumn{2}{c}{}& \multicolumn{6}{ |c| }{Predicted class}\\
	\cline{3-8}
	\multicolumn{2}{c|}{} & 1 Anger & 2 Disgust & 3 Fear & 4 Happiness & 5 Sadness & 6 Surprise\\ \cline{1-8}
	\multirow{6}{*}{Actual class}& 1 Anger & 64 & 6 & 4 & 2 & 11 & 1 \\ \cline{2-8}
	& 2 Disgust & 72 & 100 & 6 & 3 & 4 & 2\\ \cline{2-8}
	& 3 Fear & 76 & 6 & 70 & 7 & 11 & 17 \\ \cline{2-8}
	& 4 Happiness & 27 & 8 & 9 & 156 & 3 & 5 \\ \cline{2-8}
	& 5 Sadness & 50 & 2 & 7 & 2 & 42 & 7 \\ \cline{2-8}
	& 6 Surprise & 18 & 1 & 10 & 11 & 17 & 163\\ \hline
\end{tabular}
\caption{Confusion Matrix - Yule Dissimilarity - Noisy Data}
\label{tab:yuleNoisyConfusion}
\end{table}


\begin{table}[!ht]
\centering
\begin{tabular}{|l|c|c|c|c|c|c|c|}
	\cline{3-8}
	\multicolumn{2}{c}{}& \multicolumn{6}{ |c| }{Class}\\
	\cline{3-8}
	\multicolumn{2}{c|}{} & 1 Anger & 2 Disgust & 3 Fear & 4 Happiness & 5 Sadness & 6 Surprise\\ \cline{1-8}
	\multirow{2}{*}{Algorithm}& Shared AUs (Clean) & 83.9695 & 81.3131 & 80.5085 & 94.8837 & 76.5152 & 91.7476 \\ \cline{2-8}  
	& Shared AUs (Noisy) & 45.4545 & 79.1444 & 64.1711 & 78.3654 & 51.8182 & 79.5455 \\ \cline{2-8} 
		& Weighted (Clean) & 78.6260 & 87.3737 & 80.5085 & 94.8837 & 75.7576 & 93.2039\\ \cline{2-8} 
	& Weighted (Noisy) & 38.6364 & 73.7968 & 63.1016 & 77.8846 & 43.6364 & 82.7273\\ \cline{2-8}
			& Dice (Clean) & 79.3893 & 76.7677 & 73.7288 & 91.6279 & 65.1515 & 88.8350\\ \cline{2-8} 
	& Dice (Noisy) & 28.4091 & 80.2139 & 54.5455 & 69.2308 & 26.36363 & 76.3636\\ \cline{2-8}
			& Yule (Clean) & 52.6718 & 86.8687 & 72.0339 & 87.9070 & 78.7879 & 85.9223\\ \cline{2-8} 
	& Yule (Noisy) & 72.7273 & 53.4759 & 37.4332 & 75.0000 & 38.1818 & 74.0909\\ \hline

\end{tabular}
\caption{Precision Per Class}
\label{tab:precisionPerClass}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{|l|c|c|c|c|c|c|c|}
	\cline{3-8}
	\multicolumn{2}{c}{}& \multicolumn{6}{ |c| }{Class}\\
	\cline{3-8}
	\multicolumn{2}{c|}{} & 1 Anger & 2 Disgust & 3 Fear & 4 Happiness & 5 Sadness & 6 Surprise\\ \cline{1-8}
	\multirow{2}{*}{Algorithm}& Shared AUs (Clean) & 78.0142  & 79.3103 & 85.5856 & 92.3077 & 84.1667  & 92.6471 \\ \cline{2-8}  
	& Shared AUs (Noisy) & 33.3333 & 72.9064 & 63.4921 & 84.8958 & 65.5172 & 83.7321 \\ \cline{2-8} 
		& Weighted (Clean) & 75.7553 & 79.7235 & 87.9630 & 94.0092 & 83.3333 & 95.0495 \\ \cline{2-8} 
	& Weighted (Noisy) & 30.6306 & 66.0287 & 63.4409 & 85.2632 & 60.7595 & 80.8889 \\ \cline{2-8}
			& Dice (Clean) & 66.2420 & 76.7677 & 73.1092 & 90.3670 & 84.3137 & 88.8350\\ \cline{2-8} 
	& Dice (Noisy) & 25.5102 & 62.7615 & 52.3077 & 77.4194 & 59.1837 & 72.1030 \\ \cline{2-8}
			& Yule (Clean) & 81.1765  & 73.5043 & 75.8929 & 95.4545 & 57.1429 & 93.6508\\ \cline{2-8} 
	& Yule (Noisy) & 20.8469 & 81.3008 & 66.0377 & 86.1878 & 47.7273 & 83.5897\\ \hline

\end{tabular}
\caption{Recall Per Class}
\label{tab:recallPerClass}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{|l|c|c|c|c|c|c|c|}
	\cline{3-8}
	\multicolumn{2}{c}{}& \multicolumn{6}{ |c| }{Class}\\
	\cline{3-8}
	\multicolumn{2}{c|}{} & 1 Anger & 2 Disgust & 3 Fear & 4 Happiness & 5 Sadness & 6 Surprise\\ \cline{1-8}
	\multirow{2}{*}{Algorithm}& Shared AUs (Clean) & 80.8824  & 80.2993 & 82.9694 & 93.5701 & 80.1587 & 92.1951 \\ \cline{2-8}  
	& Shared AUs (Noisy) & 38.4615 & 75.8974 & 63.8298 & 81.5000 & 57.8680 & 81.5851\\ \cline{2-8} 
		& Weighted (Clean) & 77.1536 & 83.3735 & 84.0708 & 94.4444 & 79.3651 & 94.1176\\ \cline{2-8} 
	& Weighted (Noisy) & 34.1709 & 69.6970 & 63.2708 & 81.4070 & 50.7937 & 81.7978\\ \cline{2-8}
			& Dice (Clean) & 72.2222 & 76.7677 & 73.4177 & 90.9931 & 73.5043 & 88.8350 \\ \cline{2-8} 
	& Dice (Noisy) & 45.4545 & 83.7093 & 67.5393 & 85.9223 & 56.5217 & 85.0112\\ \cline{2-8}
			& Yule (Clean) & 63.8889 & 79.6296 & 739130 & 91.5254 & 66.2420 & 89.6203 \\ \cline{2-8} 
	& Yule (Noisy) & 32.4051 & 64.5161 & 47.7816 & 80.2057 & 42.4242 & 78.5542\\ \hline
	

\end{tabular}
\caption{F1 Measure Per Class}
\label{tab:f1PerClass}
\end{table}

\bibliography{case_based_reasoning}
\bibliographystyle{plain}

\end{document}
