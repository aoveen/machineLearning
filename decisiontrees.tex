\documentclass[11pt,a4paper]{article}
\usepackage[margin=1.0in]{geometry}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{subcaption}
\author{Hector Dearman \and Paul Rowe-White \and Kritaphat Songsriin \and Simon Stuckemann}
\title{Machine Learning CBC: Decision Trees}
\begin{document}
\maketitle

\section{Implementation Summary}

In order to gather statistical results of our algorithm and compare the performance of different approaches we implemented a few functions that would help us with this task.

To perform 10-fold cross validation efficiently we implemented the function \texttt{cross\_fold \_validation(x, y)} which takes examples \texttt{x} and classifications \texttt{y} and returns a confusion matrix that was found by performing several tests using 10-fold cross validation. In this function, the set of examples are split into 10 folds, leaving out any extra examples so that each fold is of the same size. In 10 iterations the algorithm then removes one of the buckets as the test set and combines the rest into a training set (no validation set is required in this part of the coursework).The training set is then used to train a decision tree using the \texttt{build\_tree(training\_data\_x, training\_data\_y, attribute} which build the decision tree for the given attribute using the given data (consisting of 9 folds). The function then tests the tree using the \texttt{testTrees} function which returns a vector of predictions. The predictions are then compared to the actual classifications and the results are added to a confusion matrix. After all values have been added from all ten folds the confusion matrix is turned into an average confusion matrix by simply dividing the values by 10 and returned to the user. 

Note that this means that any error rate or other statistics given in this report is based on the average confusion matrix and not given in terms of the average of many calculations of that statistic over the course of the cross validation. 
% elaborate here?

As part of the \texttt{build\_tree} function we implemented a function that selects the best attribute from a set of attributes on which to split the set of examples. This function (\texttt{choose\_best \_decision \_attribute(examples, attributes, binary\_targets)}) maximises the information gain of each possible split and thus returns the attribute that returns the largest information gain (n = negative examples, p = positive examples, $p_0$ = positive examples of the subset of data for which the attribute has a value 0, etc.):

\begin{align}
Gain(attribute)  &= I(p,n) - Remainder(attribute)\\
I(p,n) &= -\left(\frac{p}{p+n}\right)\log_2{\left(\frac{p}{p+n}\right)} -\left(\frac{n}{p+n}\right)\log_2{\left(\frac{n}{p+n}\right)}\\
Remainder(attribute) &= \frac{p_0 + n_0}{p+n}I(p_0,n_0) + \frac{p_1 + n_1}{p + n}I(p_1,n_1)
\end{align}
Note that in the implementation of $I(p,n)$ we check explicitly for the case when $p = 0$ or $n = 0$ as Matlab will otherwise return $NaN$ which will result in unbalanced trees.
%elaborate here maybe?

\begin{figure}[!ht]
\centering
\includegraphics[scale=0.7]{images/flowchart.pdf}
\caption{Algorithm Description}
\label{fig:flowchart}
\end{figure}

\section{Diagrams of six trees}

look at the pretty pictures

\section{Evaluation}
\subsection{Clean Dataset}
Analysis of the clean dataset results in the confusion matrix that can be seen in table \ref{tab:chooseDepthCleanConfusion}. From this confusion matrix, the recall and precision rates can be calculated which can then be combined to a single $F_1$ measure as seen in table \ref{tab:chooseDepthCleanStats}.
\begin{table}[!ht]
\centering
\begin{tabular}{|l|c|c|c|c|c|c|c|}
	\hline
	& \multicolumn{7}{ |c| }{Predicted class}\\
	\hline
	\multirow{7}{*}{Actual class} & & 1 & 2 & 3 & 4 & 5 & 6\\ \cline{2-8}
	& 1 & 8.4 & 1.8 & 0.6 & 0.4 & 1.5 & 0.4 \\ \cline{2-8}
	& 2 & 1.8 & 14.7 & 0.3 & 1.0 & 1.3 & 0.7\\ \cline{2-8}
	& 3 & 0.4 & 0.6 & 8.6 & 0.4 & 0.3 & 1.5 \\ \cline{2-8}
	& 4 & 0.1 & 1.1 & 0.3 & 18.1 & 1.4 & 0.5 \\ \cline{2-8}
	& 5 & 1.7 & 1.4 & 0.4 & 0.8 & 8.1 & 0.8 \\ \cline{2-8}
	& 6 & 0 & 0.5 & 1.5 & 0.6 & 0.9 & 17.1\\ \hline
\end{tabular}
\caption{Confusion Matrix of Final Algorithm Applied to Clean Data}
\label{tab:chooseDepthCleanConfusion}
\end{table}


\begin{table}[!ht]
\centering
\begin{tabular}{|l|c|c|c|c|c|c|}
	\hline
	& \multicolumn{6}{ |c| }{Class}\\
	\hline
	& 1 & 2 & 3 & 4 & 5 & 6\\ \hline
	Recall Rate & 64.1221 & 74.2424 & 72.8814 & 84.1860 & 61.3636 & 83.0097\\ \hline
	Precision Rate & 67.7419 & 73.1343 & 73.5043 & 84.9765 & 60.0000 & 81.4286\\ \hline
	$F_1$ & 65.8824 & 73.6842 & 73.1915 & 84.5794 & 60.6742 & 82.2115\\ \hline
\end{tabular}
\caption{Statistics for Final Algorithm Applied to Clean Data}
\label{tab:chooseDepthCleanStats}
\end{table}

In addition, the calculated error rate is $0.2500$, resulting in a classification rate of $0.7500$ (see table \ref{tab:chooseDepthRates}).

\subsection{Noisy Dataset}

Analysis of the noisy dataset results in the confusion matrix that can be seen in table \ref{tab:chooseDepthNoisyConfusion}. From this confusion matrix, the recall and precision rates can be calculated which can then be combined to a single $F_1$ measure as seen in table \ref{tab:chooseDepthNoisyStats}.

\begin{table}[!ht]
\centering
\begin{tabular}{|l|c|c|c|c|c|c|c|}
	\hline
	& \multicolumn{7}{ |c| }{Predicted class}\\
	\hline
	\multirow{7}{*}{Actual class} & & 1 & 2 & 3 & 4 & 5 & 6\\ \cline{2-8}
	& 1 & 3.1 & 0.7 & 1.7 & 0.8 & 1.9 & 0.6 \\ \cline{2-8}
	& 2 & 1.6 & 12.3 & 2.2 & 1.5 & 0.8 & 0.3\\ \cline{2-8}
	& 3 & 1.4 & 1.7 & 10.6 & 2.3 & 1.0 & 1.7 \\ \cline{2-8}
	& 4 & 0.8 & 1.1 & 1.4 & 15.7 & 0.5 & 1.3 \\ \cline{2-8}
	& 5 & 2.0 & 0.8 & 0.8 & 0.7 & 5.8 & 0.9 \\ \cline{2-8}
	& 6 & 1.1 & 0.4 & 2.0 & 0.8 & 1.1 & 16.6\\ \hline
\end{tabular}
\caption{Confusion Matrix of Final Algorithm Applied to Noisy Data}
\label{tab:chooseDepthNoisyConfusion}
\end{table}

\begin{table}
\centering
\begin{tabular}{|l|c|c|c|c|c|c|}
	\hline
	& \multicolumn{6}{ |c| }{Class}\\
	\hline
	& 1 & 2 & 3 & 4 & 5 & 6\\ \hline
	Recall Rate & 35.2273 & 65.7754 & 56.6845 & 75.4808 & 52.7273 & 75.4545 \\ \hline
	Precision Rate & 31.0000 & 72.3529 & 56.6845 & 72.0183 & 52.2523 & 77.5701\\ \hline
	$F_1$ & 32.9787 & 68.9076 & 56.6845 & 73.7089 & 52.4887 & 76.4977\\ \hline
\end{tabular}
\caption{Statistics for Final Algorithm Applied to Noisy Data}
\label{tab:chooseDepthNoisyStats}
\end{table}

In addition, the calculated error rate is $0.3590$, resulting in a classification rate of $0.6410$ (see table \ref{tab:chooseDepthRates}).

\subsection{Conclusion}

There is a difference in the performance when using the clean and noisy datasets. The clean set performs a lot better than the other. This is because while the clean dataset contains only consistent classifications, the noisy set also contains examples with their classifications that do not agree. Because of this, the trees that are constructed from this data classify examples incorrectly.


\section{Emotion Ambiguity}

During the training phase six decision trees are built, each corresponding to one of the six emotions \emph{Happiness}, \emph{Sadness}, \emph{Surprise}, \emph{Disgust}, \emph{Fear}, and \emph{Anger}. When the decision trees are then used to determine whether a given example is an example of either of these emotions it is possible that more than of the trees classifies the example positively, thus making the outcome ambiguous, since the example could now indicate multiple emotions. 

In order to return one of the emotions to the end user the algorithm needs to pick one of the positive emotions (or indeed one of the negative if none of the trees classified the example positively). To find out which approach leads to the best result we tried to use multiple algorithms and compared their performance.

\subsection{Choose First Positive Classification}

A very simple solution to the problem and arguably the easiest to implement is to always select the first positively classified emotion, or indeed the first emotion if none of the classifiers returned a positive result. In \emph{Matlab} this can be implemented by simply using the \texttt{max} function on the row vector of results. Simplicity is the main advantage of this approach: the code that implements this solution not only is easy to read but also is one of the fastest mechanisms to decide between different solutions. However, this approach also has many disadvantages. For instance, there is no inherent order in the emotions, which means that the trees corresponding to the emotions can also not be ordered. Always choosing the first occurrence of a positive classification thus seems arbitrary and results in a large skew of the distribution of classifications.

This disadvantage also shows in the statistical evaluation of this approach as we can see in tables \ref{tab:chooseFirstStats} and \ref{tab:chooseFirstRates}. It can clearly be seen that there is again a difference between the clean and noisy datasets. This is consistent with the end results discussed above and (briefly) below. It can be seen that the error rate is $27.2\%$ is relatively large for clean data. However, in noisy data the performance of this approach drops significantly to an error rate of $40.7\%$. A closer look at the individual recall and precision rates in table \ref{tab:chooseFirstStats} explains why: The precision rate, which measures the number of false positives, is extremely low with only $19.29\%$ ($F_1 = 29.65\%$) in the first classifier for the noisy data set which can be explained by the favouritism in this approach. It is also apparent that the number of false negatives (as measured by the recall rate) is is relatively large in all other classes, which can also  be explained by the skew.

\begin{table}
\centering
\begin{tabular}{|l|c|c|c|c|c|c|}
	\hline
	& \multicolumn{6}{ |c| }{Class}\\
	\hline
	& 1 & 2 & 3 & 4 & 5 & 6\\ \hline
	Recall Rate Clean & 83.9695 & 73.2323 & 66.9492 & 81.3953 & 49.2424 & 74.7573 \\ \hline
	Precision Rate Clean & 42.8016 & 76.7196 & 77.4510 & 90.2062 & 73.0337 & 91.1243\\ \hline
	$F_1$ Clean & 56.7010 & 74.9354 & 71.8182 & 85.5746 & 58.8235 & 82.1333\\ \hline \hline
	Recall Rate Noisy & 55.6818 & 67.9144 & 52.4064 & 66.8269 & 40.0000 & 61.8182 \\ \hline
	Precision Rate Noisy & 19.2913 & 71.7514 & 62.4204 & 78.9773 & 61.1111 & 82.9268\\ \hline
	$F_1$ Noisy & 28.6550 & 69.7802 & 56.9767 & 72.3958 & 48.3516 & 70.8333\\ \hline
\end{tabular}
\caption{Statistics for \emph{Choose-First} Algorithm}
\label{tab:chooseFirstStats}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|}
\hline 
 & \textbf{Error Rate} & \textbf{Classification Rate} \\ 
\hline 
Clean Data & $0.2720$ & $0.7280$ \\ 
\hline 
Noisy Data & $0.4070$ & $0.5930$ \\ 
\hline 
\end{tabular} 
\caption{Error Rate and Classification Rate in \emph{Choose-First} Algorithm}
\label{tab:chooseFirstRates}
\end{table}

\subsection{Choose Random Positive Classification}
One way to avoid the skew that we encountered in the previous approach is to choose a random positive classification from the set of positive classifications using a uniform distribution. The advantage of this approach is that it is also relatively simple to implement and fast to execute. It also does not require any more information about the tree (which, as we will see, is a requirement for our final and next approach). 
The main disadvantage of taking this approach, however, is that even though we are now choosing randomly from all the positive classifications it does not take into account that one of the classifiers may have been \emph{more certain} about the outcome. In addition to this, the resulting classification is also no longer deterministic, thus making the method return inconsistent classifications for the same example. Statistically, however, this approach improves on the previous results. As we can see in table \ref{tab:chooseRandomStats} the precision rate is generally higher for all classes (although still not great for some of the classes). Interestingly, the performance in terms of classification rate (see table \ref{tab:chooseRandomRates}) has dropped slightly when using clean data. However, one can argue that noisy data is closer to the real world, thus making the importance on this data more important. Crucially, the random algorithm performs a few percentage points better than the previous algorithm.

\begin{table}
\centering
\begin{tabular}{|l|c|c|c|c|c|c|}
	\hline
	& \multicolumn{6}{ |c| }{Class}\\
	\hline
	& 1 & 2 & 3 & 4 & 5 & 6\\ \hline
	Recall Rate Clean & 62.5954 & 70.7071 & 72.0339 & 78.6047 & 53.7879 & 80.0971 \\ \hline
	Precision Rate Clean & 63.0769 & 70.0000 & 64.8855 & 80.4762 & 58.1967 & 79.7101\\ \hline
	$F_1$ Clean & 62.8352 & 70.3518 & 68.2731 & 79.5294 & 55.9055 & 79.9031\\ \hline \hline
	Recall Rate Noisy & 22.7273 & 72.1925 & 56.1497 & 74.5192 & 45.4545 & 69.5455\\ \hline
	Precision Rate Noisy & 22.9885 & 69.9482 & 60.6936 & 68.8889 & 45.4545 & 72.1698\\ \hline
	$F_1$ Noisy & 22.8571 & 71.0526 & 58.3333 & 71.5935 & 45.4545 & 70.8333\\ \hline
\end{tabular}
\caption{Statistics for \emph{Choose-Random} Algorithm}
\label{tab:chooseRandomStats}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|}
\hline 
 & \textbf{Error Rate} & \textbf{Classification Rate} \\ 
\hline 
Clean Data & $0.2880$ & $0.7120$ \\ 
\hline 
Noisy Data & $0.3820$ & $0.6180$ \\ 
\hline 
\end{tabular} 
\caption{Error Rate and Classification Rate in \emph{Choose-Random} Algorithm}
\label{tab:chooseRandomRates}
\end{table}

\subsection{Choose Positive Classification with Shortest Classification Path}
One of the major disadvantages of the previous approaches was that both did not take into account the certainty with which the classifiers determined the positive classification of an example. This is an issue when multiple classifiers return a positive classification, one of which based their decision on an attribute with large information gain and the others basing their decision on an attribute with small information gain. To avoid this issue, we decided to take into account the length of the classification path (the number of nodes in the tree the example passed before reaching a leaf node) -- the shorter the path, the more certain the classifier; Similarly, if all examples are classified negatively, then the inverse is true: the longer the classification path, the more uncertain the classifier was when it decided to classify the example negatively. One of the disadvantages of this approach, however, is that classification now requires the function to return not only the classification but also the depth of the leaf node. This is computationally more expensive and also eliminates the possibility to optimise the classification method to only check the classification paths that result in positive leaf nodes. 

As we have already seen in tables \ref{tab:chooseDepthCleanStats} and \ref{tab:chooseDepthNoisyStats} the recall and precision rates have greatly improved. For instance, the precision rate of class $1$ in noisy data is now $31\%$, compared to the previous $22.98\%$. Overall, most recall and precision rates that were especially disappointing in previous results have improved while other values are largely the same as before, thus improving the overall performance. This can also be seen in the error rate and classification rates as seen in table \ref{tab:chooseDepthRates}.

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|}
\hline 
 & \textbf{Error Rate} & \textbf{Classification Rate} \\ 
\hline 
Clean Data & $0.2500$ & $0.7500$ \\ 
\hline 
Noisy Data & $0.3590$ & $0.6410$ \\ 
\hline 
\end{tabular} 
\caption{Error Rate and Classification Rate in \emph{Choose-Shortest-Classification-Path} Algorithm}
\label{tab:chooseDepthRates}
\end{table}

\section{Pruning}


\begin{figure}[!ht]
	\centering
	\begin{subfigure}[b]{0.495\textwidth}
		\includegraphics[width=\textwidth]{images/pruning_clean_data.eps}
     	\caption{Pruning Function Applied to Clean Data}
     	\label{fig:pruningClean}
    \end{subfigure}
	\begin{subfigure}[b]{0.495\textwidth}
		\includegraphics[width=\textwidth]{images/pruning_noisy_data.eps}
     	\caption{Pruning Function Applied to Noisy Data}
     	\label{fig:pruningNoisy}
    \end{subfigure}
\end{figure}


how does the pruning function work
two figures for clean and noisy data
explain the curves
what is the difference between the curves
what is the optimal tree size



The pruning function works by magic. The curves describe the error rate over the number of examples used to train the tree. As we can see in the figures the error rate 

what is the difference between the two curves
include 2 figures
do this for both the clean and noisy datasets

\end{document}