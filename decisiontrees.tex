\documentclass[11pt,a4paper]{article}
\usepackage[margin=1.0in]{geometry}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{multirow}
\author{Hector Dearman \and Paul Rowe-White \and Kritaphat Songsriin \and Simon Stuckemann}
\title{Machine Learning CBC: Decision Trees}
\begin{document}
\maketitle

\section{Implementation Summary}

cross validation details
How select best attribute in each node
how we compute average results:
generated confusion matrix and then calculate statistics from here

\section{Diagrams of six trees}

look at the pretty pictures

\section{Evaluation}
\subsection{Clean Dataset}
\begin{table}[!ht]
\centering
\begin{tabular}{|l|c|c|c|c|c|c|c|}
	\hline
	& \multicolumn{7}{ |c| }{Predicted class}\\
	\hline
	\multirow{7}{*}{Actual class} & & 1 & 2 & 3 & 4 & 5 & 6\\ \cline{2-8}
	& 1 & 8.4 & 1.8 & 0.6 & 0.4 & 1.5 & 0.4 \\ \cline{2-8}
	& 2 & 1.8 & 14.7 & 0.3 & 1.0 & 1.3 & 0.7\\ \cline{2-8}
	& 3 & 0.4 & 0.6 & 8.6 & 0.4 & 0.3 & 1.5 \\ \cline{2-8}
	& 4 & 0.1 & 1.1 & 0.3 & 18.1 & 1.4 & 0.5 \\ \cline{2-8}
	& 5 & 1.7 & 1.4 & 0.4 & 0.8 & 8.1 & 0.8 \\ \cline{2-8}
	& 6 & 0 & 0.5 & 1.5 & 0.6 & 0.9 & 17.1\\ \hline
\end{tabular}
\caption{Confusion Matrix of Final Algorithm Applied to Clean Data}
\end{table}


\begin{table}[!ht]
\centering
\begin{tabular}{|l|c|c|c|c|c|c|}
	\hline
	& \multicolumn{6}{ |c| }{Class}\\
	\hline
	& 1 & 2 & 3 & 4 & 5 & 6\\ \hline
	Recall Rate & 64.1221 & 74.2424 & 72.8814 & 84.1860 & 61.3636 & 83.0097\\ \hline
	Precision Rate & 67.7419 & 73.1343 & 73.5043 & 84.9765 & 60.0000 & 81.4286\\ \hline
	$F_1$ & 65.8824 & 73.6842 & 73.1915 & 84.5794 & 60.6742 & 82.2115\\ \hline
\end{tabular}
\caption{Statistics for Final Algorithm Applied to Clean Data}
\label{tab:chooseDepthCleanStats}
\end{table}

Error rate: 0.2500, Classification rate = 0.7500


\subsection{Noisy Dataset}

\begin{table}[!ht]
\centering
\begin{tabular}{|l|c|c|c|c|c|c|c|}
	\hline
	& \multicolumn{7}{ |c| }{Predicted class}\\
	\hline
	\multirow{7}{*}{Actual class} & & 1 & 2 & 3 & 4 & 5 & 6\\ \cline{2-8}
	& 1 & 3.1 & 0.7 & 1.7 & 0.8 & 1.9 & 0.6 \\ \cline{2-8}
	& 2 & 1.6 & 12.3 & 2.2 & 1.5 & 0.8 & 0.3\\ \cline{2-8}
	& 3 & 1.4 & 1.7 & 10.6 & 2.3 & 1.0 & 1.7 \\ \cline{2-8}
	& 4 & 0.8 & 1.1 & 1.4 & 15.7 & 0.5 & 1.3 \\ \cline{2-8}
	& 5 & 2.0 & 0.8 & 0.8 & 0.7 & 5.8 & 0.9 \\ \cline{2-8}
	& 6 & 1.1 & 0.4 & 2.0 & 0.8 & 1.1 & 16.6\\ \hline
\end{tabular}
\caption{Confusion Matrix of Final Algorithm Applied to Noisy Data}
\end{table}

\begin{table}
\centering
\begin{tabular}{|l|c|c|c|c|c|c|}
	\hline
	& \multicolumn{6}{ |c| }{Class}\\
	\hline
	& 1 & 2 & 3 & 4 & 5 & 6\\ \hline
	Recall Rate & 35.2273 & 65.7754 & 56.6845 & 75.4808 & 52.7273 & 75.4545 \\ \hline
	Precision Rate & 31.0000 & 72.3529 & 56.6845 & 72.0183 & 52.2523 & 77.5701\\ \hline
	$F_1$ & 32.9787 & 68.9076 & 56.6845 & 73.7089 & 52.4887 & 76.4977\\ \hline
\end{tabular}
\caption{Statistics for Final Algorithm Applied to Noisy Data}
\label{tab:chooseDepthNoisyStats}
\end{table}

Error rate: 0.3590, Classification rate = 0.6410

\subsection{Conclusion}

There is a difference in the performance when using the clean and noisy datasets. The clean set performs a lot better than the other. This is because while the clean dataset contains only consistent classifications, the noisy set also contains examples with their classifications that do not agree. Because of this, the trees that are constructed from this data classify examples incorrectly.


\section{Emotion Ambiguity}

During the training phase six decision trees are built, each corresponding to one of the six emotions \emph{Happiness}, \emph{Sadness}, \emph{Surprise}, \emph{Disgust}, \emph{Fear}, and \emph{Anger}. When the decision trees are then used to determine whether a given example is an example of either of these emotions it is possible that more than of the trees classifies the example positively, thus making the outcome ambiguous, since the example could now indicate multiple emotions. 

In order to return one of the emotions to the end user the algorithm needs to pick one of the positive emotions (or indeed one of the negative if none of the trees classified the example positively). To find out which approach leads to the best result we tried to use multiple algorithms and compared their performance.

\subsection{Choose First Positive Classification}

A very simple solution to the problem and arguably the easiest to implement is to always select the first positively classified emotion, or indeed the first emotion if none of the classifiers returned a positive result. In \emph{Matlab} this can be implemented by simply using the \texttt{max} function on the row vector of results. Simplicity is the main advantage of this approach: the code that implements this solution not only is easy to read but also is one of the fastest mechanisms to decide between different solutions. However, this approach also has many disadvantages. For instance, there is no inherent order in the emotions, which means that the trees corresponding to the emotions can also not be ordered. Always choosing the first occurrence of a positive classification thus seems arbitrary and results in a large skew of the distribution of classifications.

This disadvantage also shows in the statistical evaluation of this approach as we can see in tables \ref{tab:chooseFirstStats} and \ref{tab:chooseFirstRates}. It can clearly be seen that there is again a difference between the clean and noisy datasets. This is consistent with the end results discussed above and (briefly) below. It can be seen that the error rate is $27.2\%$ is relatively large for clean data. However, in noisy data the performance of this approach drops significantly to an error rate of $40.7\%$. A closer look at the individual recall and precision rates in table \ref{tab:chooseFirstStats} explains why: The precision rate, which measures the number of false positives, is extremely low with only $19.29\%$ ($F_1 = 29.65\%$) in the first classifier for the noisy data set which can be explained by the favouritism in this approach. It is also apparent that the number of false negatives (as measured by the recall rate) is is relatively large in all other classes, which can also  be explained by the skew.

\begin{table}
\centering
\begin{tabular}{|l|c|c|c|c|c|c|}
	\hline
	& \multicolumn{6}{ |c| }{Class}\\
	\hline
	& 1 & 2 & 3 & 4 & 5 & 6\\ \hline
	Recall Rate Clean & 83.9695 & 73.2323 & 66.9492 & 81.3953 & 49.2424 & 74.7573 \\ \hline
	Precision Rate Clean & 42.8016 & 76.7196 & 77.4510 & 90.2062 & 73.0337 & 91.1243\\ \hline
	$F_1$ Clean & 56.7010 & 74.9354 & 71.8182 & 85.5746 & 58.8235 & 82.1333\\ \hline \hline
	Recall Rate Noisy & 55.6818 & 67.9144 & 52.4064 & 66.8269 & 40.0000 & 61.8182 \\ \hline
	Precision Rate Noisy & 19.2913 & 71.7514 & 62.4204 & 78.9773 & 61.1111 & 82.9268\\ \hline
	$F_1$ Noisy & 28.6550 & 69.7802 & 56.9767 & 72.3958 & 48.3516 & 70.8333\\ \hline
\end{tabular}
\caption{Statistics for \emph{Choose-First} Algorithm}
\label{tab:chooseFirstStats}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|}
\hline 
 & \textbf{Error Rate} & \textbf{Classification Rate} \\ 
\hline 
Clean Data & $0.2720$ & $0.7280$ \\ 
\hline 
Noisy Data & $0.4070$ & $0.5930$ \\ 
\hline 
\end{tabular} 
\caption{Error Rate and Classification Rate in \emph{Choose-First} Algorithm}
\label{tab:chooseFirstRates}
\end{table}

\subsection{Choose Random Positive Classification}
One way to avoid the skew that we encountered in the previous approach is to choose a random positive classification from the set of positive classifications using a uniform distribution. The advantage of this approach is that it is also relatively simple to implement and fast to execute. It also does not require any more information about the tree (which, as we will see, is a requirement for our final and next approach). 
The main disadvantage of taking this approach, however, is that even though we are now choosing randomly from all the positive classifications it does not take into account that one of the classifiers may have been \emph{more certain} about the outcome. In addition to this, the resulting classification is also no longer deterministic, thus making the method return inconsistent classifications for the same example. Statistically, however, this approach improves on the previous results. As we can see in table \ref{tab:chooseRandomStats} the precision rate is generally higher for all classes (although still not great for some of the classes). Interestingly, the performance in terms of classification rate (see table \ref{tab:chooseRandomRates}) has dropped slightly when using clean data. However, one can argue that noisy data is closer to the real world, thus making the importance on this data more important. Crucially, the random algorithm performs a few percentage points better than the previous algorithm.

\begin{table}
\centering
\begin{tabular}{|l|c|c|c|c|c|c|}
	\hline
	& \multicolumn{6}{ |c| }{Class}\\
	\hline
	& 1 & 2 & 3 & 4 & 5 & 6\\ \hline
	Recall Rate Clean & 62.5954 & 70.7071 & 72.0339 & 78.6047 & 53.7879 & 80.0971 \\ \hline
	Precision Rate Clean & 63.0769 & 70.0000 & 64.8855 & 80.4762 & 58.1967 & 79.7101\\ \hline
	$F_1$ Clean & 62.8352 & 70.3518 & 68.2731 & 79.5294 & 55.9055 & 79.9031\\ \hline \hline
	Recall Rate Noisy & 22.7273 & 72.1925 & 56.1497 & 74.5192 & 45.4545 & 69.5455\\ \hline
	Precision Rate Noisy & 22.9885 & 69.9482 & 60.6936 & 68.8889 & 45.4545 & 72.1698\\ \hline
	$F_1$ Noisy & 22.8571 & 71.0526 & 58.3333 & 71.5935 & 45.4545 & 70.8333\\ \hline
\end{tabular}
\caption{Statistics for \emph{Choose-Random} Algorithm}
\label{tab:chooseRandomStats}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|}
\hline 
 & \textbf{Error Rate} & \textbf{Classification Rate} \\ 
\hline 
Clean Data & $0.2880$ & $0.7120$ \\ 
\hline 
Noisy Data & $0.3820$ & $0.6180$ \\ 
\hline 
\end{tabular} 
\caption{Error Rate and Classification Rate in \emph{Choose-Random} Algorithm}
\label{tab:chooseRandomRates}
\end{table}

\subsection{Choose Positive Classification with Shortest Classification Path}
One of the major disadvantages of the previous approaches was that both did not take into account the certainty with which the classifiers determined the positive classification of an example. This is an issue when multiple classifiers return a positive classification, one of which based their decision on an attribute with large information gain and the others basing their decision on an attribute with small information gain. To avoid this issue, we decided to take into account the length of the classification path (the number of nodes in the tree the example passed before reaching a leaf node) -- the shorter the path, the more certain the classifier; Similarly, if all examples are classified negatively, then the inverse is true: the longer the classification path, the more uncertain the classifier was when it decided to classify the example negatively. One of the disadvantages of this approach, however, is that classification now requires the function to return not only the classification but also the depth of the leaf node. This is computationally more expensive and also eliminates the possibility to optimise the classification method to only check the classification paths that result in positive leaf nodes. 

As we have already seen in tables \ref{tab:chooseDepthCleanStats} and \ref{tab:chooseDepthNoisyStats} the recall and precision rates have greatly improved. For instance, the precision rate of class $1$ in noisy data is now $31\%$, compared to the previous $22.98\%$. Overall, most recall and precision rates that were especially disappointing in previous results have improved while other values are largely the same as before, thus improving the overall performance. This can also be seen in the error rate and classification rates as seen in table \ref{tab:chooseDepthRates}.

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|}
\hline 
 & \textbf{Error Rate} & \textbf{Classification Rate} \\ 
\hline 
Clean Data & $0.2500$ & $0.7500$ \\ 
\hline 
Noisy Data & $0.3590$ & $0.6410$ \\ 
\hline 
\end{tabular} 
\caption{Error Rate and Classification Rate in \emph{Choose-Shortest-Classification-Path} Algorithm}
\label{tab:chooseDepthRates}
\end{table}

\section{Pruning}

The pruning function works by magic. The curves describe the error rate over the number of examples used to train the tree. As we can see in the figures the error rate 

what is the difference between the two curves
include 2 figures
do this for both the clean and noisy datasets

\end{document}